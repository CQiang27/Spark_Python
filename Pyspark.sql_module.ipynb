{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Module Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL 和 DataFrames 的重要类\n",
    "   - __pyspark.sql.SparkSession__ : DataFrame 和 SQL 功能的主入口;\n",
    "   - __pyspark.sql.DataFrame__ : 分布在命名列中的分布式数据集合;\n",
    "   - __pyspark.sql.Column__ : DataFrame 中的列表达式;\n",
    "   - __pyspark.sql.Row__ : Frame 中的一行数据;\n",
    "   - __pyspark.sql.GroupedData__ : 聚合方法,DataFrame.groupBy（）返回;\n",
    "   - __pyspark.sql.DataFrameNaFunctions__ : 处理缺失数据的方法（空值）;\n",
    "   - __pyspark.sql.DataFrameStatFunctions__ : 统计功能的方法;\n",
    "   - __pyspark.sql.functions__ : 可用于 DataFrame 的内置函数列表;\n",
    "   - __pyspark.sql.types__ : 可用的数据类型列表\n",
    "   - __pyspark.sql.Window__ : 用于处理窗口函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 class pyspark.sql.SparkSession(sparkContext, jsparkSession=None)\n",
    "\n",
    "    SparkSession可用于创建DataFrame，将DataFrame注册为表，在表上执行SQL，缓存表以及读取镶木地板文件。 要创建SparkSession，请使用以下构建器模式：\n",
    "    \n",
    "   __spark = SparkSession.builder \\ __ \n",
    "   \n",
    "   __...     .master(\"local\") \\__\n",
    "   \n",
    "   __...     .appName(\"Word Count\") \\__\n",
    "    \n",
    "   __...     .config(\"spark.some.config.option\", \"some-value\") \\__\n",
    "    \n",
    "   __...     .getOrCreate()__\n",
    "   \n",
    "- __builder__ : 具有构建 SparkSession 实例的 Builder 的类属性\n",
    " + __appName__(name)\n",
    " \n",
    "    设置应用程序的名称，该名称将显示在 Spark Web UI 中。\n",
    "\n",
    "    如果未设置应用程序名称，则将使用随机生成的名称。\n",
    "\n",
    "    参数：name - 应用程序名称\n",
    "    _\n",
    " + __config__(key=None, value=None, conf=None)\n",
    " \n",
    "    设置配置选项。 使用此方法设置的选项会自动传播到SparkConf和SparkSession自己的配置。\n",
    "\n",
    "    对于现有的SparkConf，请使用conf参数。\n",
    "    \n",
    "    Parameters:\t\n",
    "    - key – 配置属性的键名称字符串\n",
    "    - value – 配置属性的值\n",
    "    - conf – SparkConf 的一个实例\n",
    "    \n",
    " + __enableHiveSupport__()\n",
    " \n",
    "     启用 Hive 支持，包括与持久性 Hive Metastore 的连接，对 Hive serde s的支持以及 Hive 用户定义的功能。\n",
    "     \n",
    " + __getOrCreate__()\n",
    " \n",
    "     获取现有 SparkSession ，如果没有现有 SparkSession ，则根据此构建器中设置的选项创建新 SparkSession。\n",
    "\n",
    "    此方法首先检查是否存在有效的全局默认 SparkSession ，如果是，则返回该值。 如果不存在有效的全局默认 SparkSession ，则该方法将创建新的 SparkSession 并将新创建的 SparkSession 指定为全局默认值。\n",
    " \n",
    " + __master__(master)\n",
    "      \n",
    "     设置要连接的 Spark 主 URL ，例如“本地”以在本地运行，“ local [4] ”在本地运行 4 核，或“ spark：// master：7077 ”在Spark独立群集上运行。\n",
    "\n",
    "    参数：master - spark master的url\n",
    " \n",
    " + __catalog__\n",
    " \n",
    "    用户可通过其创建，删除，更改或查询底层数据库，表，函数等的接口。\n",
    "\n",
    "    返回：目录\n",
    " \n",
    " + __conf__\n",
    " \n",
    "     Spark的运行时配置界面。\n",
    "\n",
    "     这是用户可以通过该接口获取和设置与 Spark SQL 相关的所有 Spark 和 Hadoop 配置。 获取配置的值时，默认为基础 SparkContext 中设置的值（如果有）\n",
    " \n",
    " + __createDataFrame__(data, schema=None, samplingRatio=None, verifySchema=True)\n",
    " \n",
    "     从 RDD，列表或 pandas.DataFrame 创建 DataFrame。\n",
    "\n",
    "    当 schema 是列名列表时，将从数据推断每列的类型。\n",
    "\n",
    "    当 schema 为 None 时，它将尝试从数据推断出模式（列名和类型），数据应该是 Row 的 RDD，或者是 namedTuple 或 dict。\n",
    "\n",
    "    schema 是 pyspark.sql.types.DataType 或数据类型字符串时，它必须与实际数据匹配，否则将在运行时抛出异常。如果给定的模式不是 pyspark.sql.types.StructType ，它将被包装到 pyspark.sql.types.StructType 中作为其唯一的字段，并且字段名称将为“value”，每条记录也将被包装成一个元组，可以在以后转换为行。\n",
    "\n",
    "    如果需要模式推断，则 samplingRatio 用于确定用于模式推断的行的比率。如果 samplingRatio 为 None，则将使用第一行。\n",
    "\n",
    "    __参数__：\n",
    "    - data - 任何类型的 SQL 数据表示的 RDD（例如，row，tuple，int，boolean 等），或 list 或 pandas.DataFrame。\n",
    "    - schema - pyspark.sql.types.DataType 或数据类型字符串或列名列表，默认为 None。数据类型字符串格式等于 pyspark.sql.types.DataType.simpleString，除了顶级结构类型可以省略 struct <> 和原子类型使用 typeName（） 作为它们的格式，例如对 pyspark.sql.types.ByteType 使用 byte 而不是 tinyint。我们也可以使用 int 作为 IntegerType 的短名称。\n",
    "    - samplingRatio - 用于推断的行的样本比率\n",
    "    - verifySchema - 根据模式验证每一行的数据类型。\n",
    "    \n",
    "    __返回：数据帧__\n",
    "    \n",
    " + __newSession__()\n",
    "     \n",
    "     返回一个新的 SparkSession 作为新会话，它具有单独的 SQLConf，已注册的临时视图和 UDF，但共享 SparkContext 和表缓存。\n",
    " + __range__(start, end=None, step=1, numPartitions=None)\n",
    "     \n",
    "     使用名为id的单个pyspark.sql.types.LongType列创建一个DataFrame，其中包含从开始到结束（不包括）的步骤值步骤范围内的元素。\n",
    "\n",
    "    参数：\n",
    "    - start - 起始值\n",
    "    - end - 结束价值（独家）\n",
    "    - step - 增量步骤（默认值：1）\n",
    "    - numPartitions - DataFrame的分区数\n",
    "    \n",
    "    __返回：数据帧__\n",
    " \n",
    " + __read__\n",
    "     \n",
    "    返回一个 DataFrameReader，可用于以 DataFrame 的形式读取数据。\n",
    "\n",
    "    __返回：DataFrameReader__\n",
    " \n",
    " \n",
    " + __readStream__\n",
    " \n",
    "     返回一个 DataStreamReader ，可用于将数据流作为流式 DataFrame 读取。\n",
    "     \n",
    "     __返回：DataStreamReader__\n",
    " \n",
    " + __sparkContext__\n",
    " \n",
    "     返回底层的 SparkContext\n",
    "     \n",
    " + __sql__(sqlQuery)\n",
    " \n",
    "    返回表示给定查询结果的 DataFrame\n",
    "\n",
    "    __返回：数据帧__\n",
    " \n",
    " + __stop__()\n",
    "    \n",
    "    停止底层的 SparkContext\n",
    " \n",
    " + __streams__\n",
    " \n",
    "    返回一个 StreamingQueryManager，它允许管理在此上下文中活动的所有 StreamingQuery StreamingQueries。\n",
    "\n",
    "    返回：StreamingQueryManager\n",
    " \n",
    " + __table__(tableName)\n",
    " \n",
    "    将指定的表作为DataFrame返回。\n",
    "\n",
    "    __返回：数据帧__\n",
    "    \n",
    " + __udf__\n",
    " \n",
    "    返回UDF注册的UDFRegistration。\n",
    "\n",
    "    __返回：UDFRegistration__\n",
    "    \n",
    " + __version__\n",
    " \n",
    "     运行此应用程序的Spark版本。\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 pyspark.sql.SQLContext(sparkContext, sparkSession=None, jsqlContext=None)\n",
    "\n",
    "    在Spark 1.x中使用Spark中的结构化数据（行和列）的入口点。\n",
    "\n",
    "    从Spark 2.0开始，它被SparkSession取代。 但是，为了向后兼容，我们将此类保留在此处。\n",
    "\n",
    "    可以使用SQLContext创建DataFrame，将DataFrame注册为表，对表执行SQL，缓存表以及读取镶木地板文件。\n",
    "\n",
    "####     参数：\n",
    "- sparkContext - 支持此SQLContext的SparkContext。\n",
    "- sparkSession - 这个SQLContext包围的SparkSession。\n",
    "- jsqlContext - 可选的JVM Scala SQLContext。 如果设置，我们不会在JVM中实例化新的SQLContext，而是对这个对象进行所有调用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 pyspark.sql.HiveContext(sparkContext, jhiveContext=None)\n",
    "\n",
    "    Spark SQL 的一种变体，它与存储在 Hive 中的数据集成。\n",
    "\n",
    "    从类路径上的 hive-site.xml 读取 Hive 的配置。 它支持运行 SQL 和 HiveQL 命令。\n",
    "    \n",
    "   __参数：__\n",
    "   \n",
    "   - __sparkContext__ - 要包装的SparkContext。\n",
    "   \n",
    "   - __jhiveContext__ - 可选的JVM Scala HiveContext。 如果设置，我们不会在JVM中实例化新的HiveContext，而是对这个对象进行所有调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
